% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
  \setmainfont[]{Georgia}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1.0in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{geometry}
\usepackage{subcaption}
\usepackage{lineno}
\usepackage{makecell}
\usepackage{pdflscape}
\definecolor{listcomment}{rgb}{0.0,0.5,0.0}
\definecolor{listkeyword}{rgb}{0.0,0.0,0.5}
\definecolor{listnumbers}{gray}{0.65}
\definecolor{listlightgray}{gray}{0.955}
\definecolor{listwhite}{gray}{1.0}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{\vspace{-2.5em}}

\begin{document}

\setstretch{1.5}
\linenumbers
\pagenumbering{gobble}

\setstretch{1}

\begin{centering}

$ $

\vspace{6cm}

\LARGE

{\bf Modular strategies for spatial mapping of multi-modal mouse brain data}

\vspace{1.0 cm}

\normalsize

Nicholas J. Tustison$^{1}$,
Min Chen$^{2}$,
Fae N. Kronman$^{3}$,
Jeffrey T. Duda$^{2}$,
Clare Gamlin$^{4}$,
Mia G. Tustison,
Michael Kunst$^{4}$,
Rachel Dalley$^{4}$,
Staci Sorenson$^{4}$,
Quanxin Wang$^{4}$,
Lydia Ng$^{4}$,
Yongsoo Kim$^{3}$, and
James C. Gee$^{2}$

\small

$^{1}$Department of Radiology and Medical Imaging, University of Virginia, Charlottesville, VA \\
$^{2}$Department of Radiology, University of Pennsylvania, Philadelphia, PA \\
$^{3}$Department of Neural and Behavioral Sciences, Penn State University, Hershey, PA \\
$^{4}$Allen Institute for Brain Science, Seattle, WA \\

\end{centering}

\vspace{3.5 cm}

\noindent

\rule{4cm}{0.4pt}

\scriptsize

Corresponding authors:\\

Nicholas J. Tustison, DSc\\
Department of Radiology and Medical Imaging\\
University of Virginia\\
\href{mailto:ntustison@virginia.edu}{\nolinkurl{ntustison@virginia.edu}}\\

James C. Gee, PhD\\
Department of Radiology\\
University of Pennsylvania\\
\href{mailto:gee@upenn.edu}{\nolinkurl{gee@upenn.edu}}

\normalsize

\newpage

\setstretch{1.5}

\section*{Abstract}\label{abstract}
\addcontentsline{toc}{section}{Abstract}

Large-scale efforts by the BRAIN Initiative Cell Census Network (BICCN)
are generating a comprehensive reference atlas of cell types in the
mouse brain. A key challenge in this effort is mapping diverse
datasets---acquired with varied imaging, tissue processing, and
profiling methods---into shared coordinate frameworks. Here, we present
modular mapping pipelines developed using the Advanced Normalization
Tools Ecosystem (ANTsX) to align MERFISH spatial transcriptomics and
high-resolution fMOST morphology data to the Allen Common Coordinate
Framework (CCFv3), and developmental MRI and LSFM data to the
Developmental CCF (DevCCF). Simultaneously, we introduce two novel
methods: 1) a velocity field--based approach for continuous
interpolation across developmental timepoints, and 2) a deep learning
framework for automated brain parcellation using minimally annotated and
publicly available data. All workflows are open-source and reproducible.
We also provide general guidance for selecting appropriate strategies
across modalities, enabling researchers to adapt these tools to new
data.

\clearpage

\section{Introduction}\label{introduction}

Over the past decade, there have been significant advancements in
mesoscopic single-cell analysis of the mouse brain. It is now possible
to track single neurons\textsuperscript{1}, observe whole-brain
developmental changes at cellular resolution\textsuperscript{2},
associate brain regions with genetic composition\textsuperscript{3}, and
locally characterize neural connectivity\textsuperscript{4}. These
scientific achievements have been propelled by high-resolution profiling
and imaging techniques that enable submicron, multimodal,
three-dimensional characterizations of whole mouse brains. Among these
are micro-optical sectioning tomography\textsuperscript{5,6}, tissue
clearing methods\textsuperscript{1,7}, spatial
transcriptomics\textsuperscript{8,9}, and single-cell genomic
profiling\textsuperscript{10}, each offering expanded specificity and
resolution for cell-level brain analysis.

Recent efforts by the NIH BRAIN Initiative have mobilized large-scale
international collaborations to create a comprehensive reference
database of mouse brain structure and function. The BRAIN Initiative
Cell Census Network has aggregated over 40 multimodal datasets from more
than 30 research groups\textsuperscript{11}, many of which are
registered to standardized anatomical coordinate systems to support
integrated analysis. Among the most widely used of these frameworks is
the Allen Mouse Brain Common Coordinate Framework
(CCFv3)\textsuperscript{12}. Other CCFs include modality-specific
references\textsuperscript{13--15} and developmental
atlases\textsuperscript{16,17} that track structural change across time.

\subsection{Mouse brain mapping
challenges}\label{mouse-brain-mapping-challenges}

Robust mapping of cell type data into CCFs is essential for integrative
analysis of morphology, connectivity, and molecular identity. However,
each modality poses unique challenges. For example, differences in
tissue processing, imaging protocols, and anatomical completeness often
introduce artifacts such as distortion, tearing, holes, and signal
dropout\textsuperscript{18--23}. Intensity differences and partial
representations of anatomy can further complicate alignment. Given this
diversity specialized strategies are often needed to address the unique,
modality-specific challenges.

Existing mapping solutions fall into three broad categories. The first
includes integrated processing platforms that provide users with mapped
datasets (e.g., Allen Brain Cell Atlas\textsuperscript{24}, Brain
Architecture Portal\textsuperscript{25},
OpenBrainMap\textsuperscript{26}, and Image and Multi-Morphology
Pipeline\textsuperscript{27}). These offer convenience and high-quality
curated data, but limited generalizability and customization. The second
category involves highly specialized pipelines tailored to specific
modalities such as histology\textsuperscript{28--30}, magnetic resonance
imaging (MRI)\textsuperscript{31--33}, microCT\textsuperscript{34,35},
light sheet fluorescence microscopy (LSFM)\textsuperscript{36,37},
flourescence micro-optical sectioning tomography
(fMOST)\textsuperscript{15,38}, and spatial transcriptomics, including
multiplexed error-robust fluorescence in situ hybridization
(MERFISH)\textsuperscript{39--41}. While effective, these solutions
often require extensive engineering effort to adapt to new datasets or
modalities. Finally, general-purpose toolkits such as
elastix\textsuperscript{42}, Slicer3D\textsuperscript{43}, and the
Advanced Normalization Tools Ecosystem (ANTsX)\textsuperscript{44} have
all been applied to mouse brain mapping scenarios
(e.g.,\textsuperscript{45}). These toolkits support modular workflows
that can be flexibly composed from reusable components, offering a
powerful alternative to rigid, modality-specific solutions. However,
their use often requires familiarity with pipeline modules, parameter
tuning, and tool-specific conventions which can limit adoption.

Building on this third category, we describe a set of modular,
ANTsX-based pipelines specifically tailored for mapping diverse mouse
brain data into standardized anatomical frameworks. These include two
new pipelines: a velocity field--based interpolation model that
potentially enables biologically plausible transformations across
developmental timepoints, and a template-based deep learning pipeline
for brain extraction and parcellation requiring minimal annotated data.
In addition, we include two modular pipelines for aligning multiplexed
error-robust fluorescence in situ hybridization (MERFISH) and fMOST
datasets to the Allen CCFv3. These workflows were adapted and tailored
using ANTsX tools to support collaborative efforts within the BICCN and
are now made openly available in a reproducible format. To facilitate
broader adoption, we also provide general guidance for customizing these
strategies across imaging modalities and data types. We first introduce
key components of the ANTsX toolkit, which provide a basis for all of
the mapping workflows described here, and then detail the specific
contributions made in each pipeline.

\subsection{The Advanced Normalization Tools Ecosystem
(ANTsX)}\label{the-advanced-normalization-tools-ecosystem-antsx}

The Advanced Normalization Tools Ecosystem (ANTsX) has been used in a
number of applications for mapping mouse brain data as part of core
processing steps in various workflows\textsuperscript{30,46--49},
particularly its pairwise, intensity-based image registration
capabilities\textsuperscript{50} and bias field
correction\textsuperscript{51}. Historically, ANTsX development is based
on foundational approaches to image mapping\textsuperscript{52--54},
especially in the human brain, with key contributions such as the
Symmetric Normalization (SyN) algorithm\textsuperscript{50}. It has been
independently evaluated in diverse imaging domains including multi-site
brain MRI\textsuperscript{55}, pulmonary CT\textsuperscript{56}, and
multi-modal brain tumor registration\textsuperscript{57}.

Beyond registration, ANTsX provides functionality for template
generation\textsuperscript{58}, intensity-based
segmentation\textsuperscript{59}, preprocessing\textsuperscript{51,60},
and deep learning\textsuperscript{44}. It has demonstrated strong
performance in consensus labeling\textsuperscript{61}, brain tumor
segmentation\textsuperscript{62}, and cardiac motion
estimation\textsuperscript{63}. Built on the Insight Toolkit
(ITK)\textsuperscript{64}, ANTsX benefits from open-source contributions
while supporting continued algorithm evaluation and innovation. In the
context of mouse brain data, ANTsX provides a robust platform for
developing modular pipelines to map diverse imaging modalities into
CCFs. This paper highlights its use across distinct BICCN projects such
as spatial transcriptomic data from MERFISH, structural data from fMOST,
and multimodal developmental data from LSFM and MRI. We describe both
shared infrastructure and targeted strategies adapted to the specific
challenges of each modality.

\subsection{Novel ANTsX-based open-source
contributions}\label{novel-antsx-based-open-source-contributions}

We introduce two novel contributions to ANTsX developed as part of
collabortive efforts in creating the Developmental Common Coordinate
Framework (DevCCF)\textsuperscript{16}. First, we present an open-source
velocity field--based interpolation framework for continuous mapping
across the sampled embryonic and postnatal stages of the DevCCF
atlas\textsuperscript{16}. This functionality enables biologically
plausible interpolation between timepoints via a time-parameterized
diffeomorphic velocity model\textsuperscript{65}, inspired by previous
work\textsuperscript{66}. Second, we present a deep learning pipeline
for structural parcellation of the mouse brain from multimodal MRI data.
This includes two novel components: 1) a template-derived brain
extraction model using augmented data from two ANTsX-derived template
datasets\textsuperscript{67,68}, and 2) a template-derived parcellation
model trained on DevCCF P56 labelings mapped from the AllenCCFv3. This
pipeline demonstrates how ANTsX tools and public resources can be
leveraged to build robust anatomical segmentation pipelines with minimal
annotated data. We independently evaluate this framework using a
longitudinal external dataset\textsuperscript{69}, demonstrating
generalizability across specimens and imaging protocols. All components
are openly available through the R and Python ANTsX packages, with
general-purpose functionality documented in a reproducible,
cross-platform tutorial (\url{https://tinyurl.com/antsxtutorial}). Code
specific to this manuscript, including scripts to reproduce the novel
contributions and all associated evaluations, is provided in a dedicated
repository (\url{https://github.com/ntustison/ANTsXMouseBrainMapping}).
Additional tools for mapping spatial transcriptomic (MERFISH) and
structural (fMOST) data to the AllenCCFv3 are separately available at
(\url{https://github.com/dontminchenit/CCFAlignmentToolkit}).

\clearpage
\newpage

\section{Results}\label{results}

\begin{figure*}
\centering
\begin{subfigure}[t]{0.49\textwidth}
\centering
\includegraphics[width=0.99\textwidth]{Figures/merfishPipeline.pdf}
\caption{}
\end{subfigure} 
\begin{subfigure}[t]{0.49\textwidth}
\centering
\includegraphics[width=0.99\textwidth]{Figures/fmostPipeline.pdf}
\caption{}
\end{subfigure}
\caption{Diagram of the two ANTsX-based pipelines for mapping (a) MERFISH
          and (b)fMOST data into the space of AllenCCFv3.  Each generates
         the requisite transforms, $\mathcal{T}$, to map individual images
         to the CCF.}
\label{fig:allenpipelines}
\end{figure*}

\subsection{AllenCCFv3 brain image
mapping}\label{allenccfv3-brain-image-mapping}

\subsubsection{Mapping multiplexed error-robust fluorescence in situ
hybridization (MERFISH)
data}\label{mapping-multiplexed-error-robust-fluorescence-in-situ-hybridization-merfish-data}

\textbf{Overview.} The ANTsX framework was used to develop a pipeline
for mapping multiplexed error-robust fluorescence in situ hybridization
(MERFISH) spatial transcriptomic mouse data onto the AllenCCFv3 (see
Figure \ref{fig:allenpipelines}(a)). This pipeline, used recently in
creating a high-resolution transcriptomic atlas of the mouse
brain\textsuperscript{49}, performs mappings by first generating
anatomical labels from tissue related gene expressions in the MERFISH
data, and then spatially matching these labels to corresponding
anatomical tissue parcellations in the AllenCCFv3. The pipeline consists
of MERFISH data specific preprocessing which includes section
reconstruction, mapping corresponding anatomical labels between
AllenCCFv3 and the spatial transcriptomic maps of the MERFISH data, and
matching MERFISH sections to the atlas space. Following preprocessing,
two main alignment steps were performed: 1) 3-D global affine mapping
and section matching of the AllenCCFv3 into the MERFISH data and 2) 2-D
global and deformable mapping between each MERFISH section and matched
AllenCCFv3 section. Mappings learned via each step in the pipeline are
preserved and concatenated to provide point-to-point correspondence
between the original MERFISH data and AllenCCFv3, thus allowing
individual gene expressions to be transferred into the AllenCCFv3.

\textbf{Data.} MERFISH mouse brain data was acquired using a previously
detailed procedure\textsuperscript{49}. Briefly, a brain of C57BL/6
mouse was dissected according to standard procedures and placed into an
optimal cutting temperature (OCT) compound (Sakura FineTek 4583) in
which it was stored at -80\(^\circ\)C. The fresh frozen brain was
sectioned at \(10 \mu m\) on Leica 3050 S cryostats at intervals of
\(200 \mu m\) to evenly cover the brain. A set of 500 genes were imaged
that had been carefully chosen to distinguish the \({\sim}5200\)
clusters of our existing RNAseq taxonomy. For staining the tissue with
MERFISH probes, a modified version of instructions provided by the
manufacturer was used\textsuperscript{49}. Raw MERSCOPE data were
decoded using Vizgen software (v231). Cells were segmented based on DAPI
and PolyT staining using Cellpose\textsuperscript{70,71}. Segmentation
was performed on a median z-plane (fourth out of seven) and cell borders
were propagated to z-planes above and below. To assign cluster identity
to each cell in the MERFISH dataset, we mapped the MERFISH cells to the
scRNA-seq reference taxonomy.

\textbf{Evaluation.} Alignment of the MERFISH data into the AllenCCFv3
was qualitatively assessed by an expert anatomist at each iteration of
the registration using known correspondence of gene markers and their
associations with the AllenCCFv3. As previously
reported\textsuperscript{49}, further assessment of the alignment showed
that, of the 554 terminal regions (gray matter only) in the AllenCCFv3,
only seven small subregions were missed from the MERFISH dataset:
frontal pole, layer 1 (FRP1), FRP2/3, FRP5; accessory olfactory bulb,
glomerular layer (AOBgl); accessory olfactory bulb, granular layer
(AOBgr); accessory olfactory bulb, mitral layer (AOBmi); and accessory
supraoptic group (ASO).

\subsubsection{Mapping fluorescence micro-optical sectioning tomography
(fMOST)
data}\label{mapping-fluorescence-micro-optical-sectioning-tomography-fmost-data}

\textbf{Overview.} We developed a pipeline for mapping fluorescence
micro-optical sectioning tomography (fMOST) mouse brain images into the
AllenCCFv3 (see Figure \ref{fig:allenpipelines}(b)). The pipeline is
adapted from previously developed frameworks for human brain
mapping\textsuperscript{58}, and uses a modality specific (fMOST)
average atlas to assist in the image registration and mapping. This
approach has been well validated in human
studies\textsuperscript{72--74}, and successfully used in other mouse
data\textsuperscript{12,15,75}. Briefly, we construct an intensity- and
shape-based average fMOST atlas using 30 fMOST images to serve as an
intermediate registration target for mapping fMOST images from
individual specimens into the AllenCCFv3. Preprocessing steps include
downsampling to match the \(25 \mu m\) isotropic AllenCCFv3,
acquisition-based stripe artifact removal, and inhomogeneity
correction\textsuperscript{51}. Preprocessing also includes a single
annotation-driven registration to establish a canonical mapping between
the fMOST atlas and the AllenCCFv3. This step allows us to align expert
determined landmarks to accurately map structures with large
morphological differences between the modalities, which are difficult to
address using standard approaches. Once this canonical mapping is
established, standard intensity-based registration is used to align each
new fMOST image to the fMOST specific atlas. This mapping is
concatenated with the canonical fMOST atlas-to-AllenCCFv3 mapping to
further map each individual brain into the latter without the need to
generate additional landmarks. Transformations learned through this
mapping can be applied to single neuron reconstructions from the fMOST
images to evaluate neuronal distributions across different specimens
into the AllenCCFv3 for the purpose of cell census analyses.

\textbf{Data.} The high-throughput and high-resolution fluorescence
micro-optical sectioning tomography (fMOST)\textsuperscript{76,77}
platform was used to image 55 mouse brains containing gene-defined
neuron populations, with sparse transgenic
expression\textsuperscript{78,79}. In short, the fMOST imaging platform
results in 3-D images with voxel sizes of
\(0.35 \times 0.35 \times 1.0 \mu m^3\) and is a two-channel imaging
system where the green channel displays the green fluorescent protein
(GFP) labeled neuron morphology and the red channel is used to visualize
the counterstained propidium iodide cytoarchitecture. The spatial
normalizations described in this work were performed using the red
channel, which offered higher tissue contrast for alignment, although
other approaches are possible including multi-channel registration.

\textbf{Evaluation.} Evaluation of the canonical fMOST atlas to Allen
CCFv3 mapping was performed via quantitative comparison at each step of
the registration and qualitative assessment of structural correspondence
after alignment by an expert anatomist. Dice values were generated for
the following structures: whole brain, 0.99; fimbria, 0.91; habenular
commissure, 0.63; posterior choroid plexus, 0.93; anterior choroid
plexus, 0.96; optic chiasm, 0.77; caudate putamen, 0.97. Similar
qualitative assessment was performed for each fMOST specimen including
the corresponding neuron reconstruction data.

\subsection{Continuously mapping the DevCCF developmental trajectory
with a velocity flow
model}\label{continuously-mapping-the-devccf-developmental-trajectory-with-a-velocity-flow-model}

\begin{figure}
\centering
\includegraphics[width=0.99\textwidth]{Figures/lowerLeftPanel.pdf}
\caption{The spatial transformation between any two time points within the
continuous DevCCF longitudinal developmental trajectory is available through the
use of ANTsX functionality for generating a velocity flow model.}
\label{fig:devccfvelocity}
\end{figure}

The DevCCF is an openly accessible resource for the mouse brain research
community\textsuperscript{80}. It consists of multi-modal MRI and LSFM
symmetric ANTsX-generated templates\textsuperscript{58} sampling the
mouse brain developmental trajectory, specifically the embryonic (E) and
postnatal (P) days E11.5, E13.5, E15.5, E18.5 P4, P14, and P56. Each
template space includes structural labels defined by a developmental
ontology. Its utility is also enhanced by a coordinated construction
with AllenCCFv3. Although this work represents a significant
contribution, the gaps between time points potentially limit its
applicability which could be addressed through the development of the
ability to map not only between time points but also within and across
time points.

To continuously generate transformations between the different stages of
the DevCCF atlases, we developed a general velocity flow model approach
which we apply to DevCCF-derived data. We also introduce this
functionality into both the ANTsR and ANTsPy packages (for the latter,
see \texttt{ants.fit\_time\_varying\_transform\_to\_point\_sets(...)})
for potential application to this and other analagous scenarios (e.g.,
modeling the cardiac and respiratory cycles). ANTsX, being built on top
of ITK, uses an ITK image data structure for the 4-D velocity field
where each voxel contains the \(x\), \(y\), \(z\) components of the
field at that point.

\subsubsection{Data}\label{data}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.75\textwidth]{Figures/SimplifiedAnnotations.pdf}
\caption{Annotated regions representing common labels across developmental stages which
are illustrated for both P4 and P14.}
\label{fig:simplifiedannotations}
\end{figure}

Labeled annotations are available as part of the original DevCCF and
reside in the space of each developmental template which range in
resolution from \(31.5-50 \mu\)m. Across all atlases, the total number
of labeled regions exceeds 2500. From these labels, a common set of 26
labels (13 per hemisphere) across all atlases were used for optimization
and evaluation. These simplified regions include: terminal hypothalamus,
subpallium, pallium, peduncular hypothalamus, prosomere, prosomere,
prosomere, midbrain, prepontine hindbrain, pontine hindbrain,
pontomedullary hindbrain, medullary hindbrain, and tracts (see Figure
\ref{fig:simplifiedannotations}).

Prior to velocity field optimization, all data were rigidly transformed
to DevCCF P56 using the centroids of the common label sets. In order to
determine the landmark correspondence across DevCCF stages, the
multi-metric capabilities of \texttt{ants.registration(...)} were used.
Instead of performing intensity-based pairwise registration directly on
these multi-label images, each label was used to construct a separate
fixed and moving image pair resulting in a multi-metric registration
optimization scenario involving 24 binary image pairs (each label
weighted equally) for optimizing diffeomorphic correspondence between
neighboring time point atlases using the mean squares metric and the
symmetric normalization transform\textsuperscript{50}.

To generate the set of common point sets across all seven developmental
atlases, the label boundaries and whole regions were sampled in the P56
atlas and then propagated to each atlas using the transformations
derived from the pairwise registrations. We selected a sampling rate of
10\% for the contour points and 1\% for the regional points for a total
number of points being per atlas being \(173303\)
(\(N_{contour} = 98151\) and \(N_{region}=75152\)). Regional boundary
points were weighted twice as those of non-boundary points during
optimization.

\subsubsection{Velocity field
optimization}\label{velocity-field-optimization}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.99\textwidth]{Figures/convergence.pdf}
\caption{Convergence of the optimization of the velocity field for describing
the transformation through the developmental stages from E11.5 through P56.
Integration points in diagram on the right are color-coordinated with the center
plot and placed in relation to the logarithmically situated temporal placement
of the individual DevCCF atlases.}
\label{fig:convergence}
\end{figure}

The velocity field was optimized using the input composed of the seven
corresponding point sets and their associated weight values, the
selected number of integration points for the velocity field (\(N=11\)),
and the parameters defining the geometry of the spatial dimensions of
the velocity field. Thus, the optimized velocity field described here is
of size \([256, 182, 360]\) (\(50 \mu\)m isotropic) \(\times 11\)
integration points for a total compressed size of a little over 2 GB.
This choice represented weighing the trade-off between tractability,
portability, and accuracy. However, all data and code to reproduce the
results described are available in the dedicated GitHub repository.

The normalized time point scalar value for each atlas/point-set in the
temporal domains \([0, 1]\) was also defined. Given the increasingly
larger gaps in the postnatal time point sampling, we made two
adjustments. Based on known mouse brain development, we used 28 days for
the P56 data. We then computed the log transform of the adjusted set of
time points prior to normalization between 0 and 1 (see the right side
of Figure \ref{fig:convergence}). This log transform, as part of the
temporal normalization, significantly improves the temporal spacing of
data.

The maximum number of iterations was set to 200 with each iteration
taking approximately six minutes on a 2020 iMac (processor, 3.6 GHz
10-Core Intel Core i9; memory, 64 GB 2667 MHz DDR4) At each iteration we
looped over the 11 integration points. At each integration point, the
velocity field estimate was updated by warping the two immediately
adjacent point sets to the integration time point and determining the
regularized displacement field between the two warped point sets. As
with any gradient-based descent algorithm, this field was multiplied by
a small step size (\(\delta = 0.2\)) before adding to the current
velocity field. Convergence is determined by the average displacement
error over each of the integration points. As can be seen in the left
panel of Figure \ref{fig:convergence}, convergence occurred around 125
iterations when the average displacement error over all integration
points is minimized. The median displacement error at each of the
integration points also trends towards zero but at different rates.

\subsubsection{The velocity flow transformation
model}\label{the-velocity-flow-transformation-model}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{Figures/CrossWarp.pdf}
\caption{Mid-sagittal visualization of the effects of the transformation model in
warping every developmental stage to the time point of every other developmental
stage.  The original images are located along the diagonal.  Columns correspond
to the warped original image whereas the rows represent the reference space to which
each image is warped.}
\label{fig:crosswarp}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{Figures/pseudo_template.pdf}
\caption{Illustration of the use of the velocity flow model for creating virtual templates
at continuous time points not represented in one of the existing DevCCF time points.
For example, FA templates at time point P10.3 and P20 can be generated by warping the 
existing temporally adjacent developmental templates to the target time point and using 
those images in the ANTsX template building process.}
\label{fig:virtual}
\end{figure}

Once optimized, the resulting velocity field can be used to generate the
deformable transform between any two continuous points within the time
interval bounded by E11.5 and P56. As a demonstration, in Figure
\ref{fig:crosswarp}, we transform each atlas to the space of every other
atlas using the DevCCF transform model. Additionally, one can use this
transformation model to construct virtual templates in the temporal gaps
of the DevCCF. Given an arbitrarily chosen time point within the
normalized time point interval, the existing adjacent DevCCF atlases on
either chronological side can be warped to the desired time point. A
subsequent call to one of the ANTsX template building functions then
permits the construction of the template at that time point. In Figure
\ref{fig:virtual}, we illustrate the use of the DevCCF velocity flow
model for generating two such virtual templates for two arbitrary time
points. Note that both of these usage examples can be found in the
GitHub repository previously given.

\subsection{Automated structural parcellations of the mouse
brain}\label{automated-structural-parcellations-of-the-mouse-brain}

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{Figures/mousePipeline.pdf}
\caption{The mouse brain cortical parcellation pipeline integrating two deep
learning components for brain extraction and brain parcellation prior to
estimating cortical labels. Both deep learning networks rely heavily on
aggressive data augmentation on templates built from open data and provide an
outline for further refinement and creating alternative parcellations for
tailored research objectives.  Possible applications include
voxelwise cortical thickness measurements.}
\label{fig:mouseKK}
\end{figure}

Brain parcellation strategies for the mouse brain are pivotal for
understanding the complex organization and function of murine nervous
system\textsuperscript{81}. By dividing the brain into distinct regions
based on anatomical, physiological, or functional characteristics,
researchers can investigate specific areas in isolation and identify
their roles in various behaviors and processes. For example, such
parcellation schemes can help elucidate the spatial distribution of gene
expression patterns\textsuperscript{82} as well as identify functional
regions involved in specific cognitive tasks\textsuperscript{83}.

Although deep learning techniques have been used to develop useful
parcellation tools for human brain research (e.g.,
SynthSeg\textsuperscript{84}, ANTsXNet\textsuperscript{44}), analogous
development for the mouse brain is limited. In addition, mouse data is
often characterized by unique imaging issues such as extreme anisotropic
sampling which are often in sharp contrast to the high resolution
template-based resources available within the community, e.g.,
AllenCCFv3 and DevCCF. We demonstrate how one can use the ANTsX tools to
develop a complete mouse brain structural morphology pipeline as
illustrated in Figure \ref{fig:mouseKK} and detailed below.

\subsubsection{Few-shot mouse brain extraction
network}\label{few-shot-mouse-brain-extraction-network}

In order to create a generalized mouse brain extraction network, we
built whole-head templates from two publicly available datasets. The
Center for Animal MRI (CAMRI) dataset\textsuperscript{67} from the
University of North Carolina at Chapel Hill consists of 16 T2-w MRI
volumes of voxel resolution \(0.16 \times 0.16 \times 0.16 mm^3\). The
second high-resolution dataset\textsuperscript{68} comprises 88
specimens each with three spatially aligned canonical views with
in-plane resolution of \(0.08 \times 0.08 mm^2\) with a slice thickness
of \(0.5 mm\). These three orthogonal views were used to reconstruct a
single high-resolution volume per subject using a B-spline fitting
algorithm available in ANTsX\textsuperscript{85}.

From these two datasets, two ANTsX templates\textsuperscript{58} were
generated. Bias field simulation, intensity histogram warping, noise
simulation, random translation and warping, and random anisotropic
resampling in the three canonical directions were used for data
augmentation in training an initial T2-w brain extraction network. This
network was posted and the corresponding functionality was immediately
made available within ANTsXNet, similar to our previous contributions to
the community.

User interest led to a GitHub inquiry regarding possible study-specific
improvements (\url{https://github.com/ANTsX/ANTsPyNet/issues/133}). This
interaction led to the offering of a user-made third template and
extracted brian mask generated from T2-w ex-vivo data with isotropic
spacing of 0.08 mm in each voxel dimension. This third template, in
conjunction with the other two, were used with the same aggressive data
augmentation to refine the network weights which were subsequently
posted and made available through ANTsPyNet using the function
\texttt{antspynet.mouse\_brain\_extraction(...)}.

\subsubsection{Single-shot mouse brain parcellation
network}\label{single-shot-mouse-brain-parcellation-network}

AllenCCFv3 and its hierarchical ontological labeling, along with the
DevCCF, provides the necessary data for developing a tailored structural
parcellation network for multi-modal imaging. The \texttt{allensdk}
Python library permits the creation of any gross parcellation based on
the AllenCCFv3 ontology. Specifically, using \texttt{allensdk} we
coalesced the labels to the following six major structures: cerebral
cortex, cerebral nuclei, brain stem, cerebellum, main olfactory bulb,
and hippocampal formation. This labeling was mapped to the P56 component
of the DevCCF for use with the T2-w template component.

The T2-w P56 DevCCF and labelings, in conjunction with the data
augmentation described previously for brain extraction, were used to
train the proposed brain parcellation network. This is available in
ANTsXNet (e.g.~in ANTsPyNet using
\texttt{antspynet.mouse\_brain\_parcellation(...)}). Note that other
brain parcellation networks have also been trained using alternative
regions and parcellation schemes and are available in the same ANTsXNet
functionality. One usage note is that the data augmentation used to
train the network permits a learned interpolation in 0.08 mm isotropic
space. Since the training data is isotropic and data augmentation
includes downsampling in the canonical directions, each of the two
networks learns mouse brain-specific interpolation such that one can
perform prediction on thick-sliced images, as, for example, in these
evaluation data, and return isotropic probability and thickness maps (a
choice available to the user). This permits robust cortical thickness
estimation even in the case of anisotropic data (see
\texttt{antspynet.mouse\_cortical\_thickness(...)}).

\subsubsection{Evaluation}\label{evaluation}

\begin{figure}
\centering
  \includegraphics[width=0.75\textwidth]{Figures/diceWholeBrain.png}
\caption{Evaluation of the ANTsX mouse brain extraction on an
independent, publicly available dataset consisting of 12 specimens $\times$ 7
time points = 84 total images.  Dice overlap comparisons with the
user-generated brain masks provide good agreement with the automated results
from the brain extraction network.}
\label{fig:evaluation}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{0.25\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/AllenCCFv3_parcellation_slice91.png}
  \caption{}
  \label{fig:subp_a}
\end{subfigure}
\begin{subfigure}{0.25\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/NR5_M_Day0_slice53.png}
  \caption{}
  \label{fig:subp_b}
\end{subfigure} \\
\begin{subfigure}{.75\textwidth}
  \centering
  \includegraphics[width=\linewidth]{Figures/diceAllenCCFv3.png}
  \caption{}
  \label{fig:subc}
\end{subfigure}
\caption{Evaluation of the ANTsX mouse brain parcellation on the same dataset.
(a) T2-w DevCCF P56 with the described parcellation consisting of the cerebral
cortex, nuclei, brain stem, cerebellum, main olfactory bulb, and hippocampal
formation. (b) Sample subject (NR5 Day 0) with the proposed deep learning-based
segmentation. (c) Dice overlap for comparing the regional alignments between
registration using intensity information only and using intensity with the given
parcellation scheme.}
\label{fig:evaluationParcellation}
\end{figure}

For evaluation, we used an additional publicly available
dataset\textsuperscript{69} that is completely independent from the data
used in training the brain extraction and parcellation networks. Data
includes 12 specimens each imaged at seven time points (Day 0, Day 3,
Week 1, Week 4, Week 8, Week 20) with in-house-generated brain masks for
a total of 84 images. Spacing is anistropic with an in-plane resolution
of \(0.1 \times 0.1 mm^2\) and a slice thickness of \(0.5 mm\).

Figure \ref{fig:evaluation} summarizes the whole brain overlap between
the provided segmentations for all 84 images and the results of applying
the proposed network. Also, since mapping to the AllenCCFv3 atlas is
crucial for many mouse studies, we demonstrate the utility of the second
network by leveraging the labeled regions to perform
anatomically-explicit alignment using ANTsX multi-component registration
instead of intensity-only registration. For these data, the whole brain
extraction demonstrates excellent performance across the large age
range. And although the intensity-only image registration provides
adequate alignment, intensity with the regional parcellations
significantly improves those measures.

\clearpage
\newpage

\section{Discussion}\label{discussion}

The diverse mouse brain cell type profiles gathered through BICCN and
associated efforts provide a rich multi-modal resource to the research
community. However, despite significant progress, optimal leveraging of
these valuable resources remains an ongoing challenge. A central
component to data integration is accurately mapping novel cell type data
into common coordinate frameworks (CCFs) for subsequent processing and
analysis. To meet these needs, tools for mapping mouse brain data must
be both broadly accessible and capable of addressing challenges unique
to each modality. In this work, we described modular ANTsX-based
pipelines developed to support three distinct BICCN efforts encompassing
spatial transcriptomic, morphological, and developmental data. We
demonstrated how a flexible image analysis toolkit like ANTsX can be
tailored to address specific modality-driven constraints by leveraging
reusable, validated components.

The MERFISH mapping pipeline illustrates how ANTsX tools can be adapted
to accommodate high-resolution spatial transcriptomic data. While the
general mapping strategy is applicable to other sectioned histological
data, the pipeline includes specific adjustments for known anatomical
and imaging artifacts present in MERFISH datasets. As such, this example
demonstrates how general-purpose tools can be customized to meet the
requirements of highly specialized data types.

The fMOST mapping pipeline was developed with the intention of broader
applicability. Built primarily from existing ANTsX preprocessing and
registration modules, this pipeline introduces an fMOST-specific
intermediate atlas to facilitate consistent mappings to the AllenCCFv3.
The use of a canonical fMOST atlas reduces the need for repeated manual
alignment across new datasets, and the resulting transformations can be
directly applied to associated single-neuron reconstructions. This
supports integrative morphological analysis across specimens using a
common coordinate system.

For developmental data, we introduced a velocity field--based model for
continuous interpolation between discrete DevCCF timepoints. Although
the DevCCF substantially expands coverage of developmental stages
relative to prior atlases, temporal gaps remain. The velocity model
enables spatio-temporal transformations within the full developmental
interval and supports the generation of virtual templates at unsampled
ages. This functionality is built using ANTsX components for velocity
field optimization and integration, and offers a novel mechanism for
interpolating across the non-linear developmental trajectory of the
mouse brain. Such interpolation has potential utility for both
anatomical harmonization and longitudinal analyses.

We also introduced a template-based deep learning pipeline for mouse
brain extraction and parcellation using aggressive data augmentation.
This approach is designed to reduce the reliance on large annotated
training datasets, which remain limited in the mouse imaging domain.
Evaluation on independent data demonstrates promising generalization,
though further refinement will be necessary. As with our human-based
ANTsX pipelines, failure cases can be manually corrected and recycled
into future training cycles. Community contributions are welcomed and
encouraged, providing a pathway for continuous improvement and
adaptation to new datasets.

The ANTsX ecosystem offers a powerful foundation for constructing
scalable, reproducible pipelines for mouse brain data analysis. Its
modular design and multi-platform support enable researchers to develop
customized workflows without extensive new software development. The
widespread use of ANTsX components across the neuroimaging community
attests to its utility and reliability. As a continuation of the BICCN
program, ANTsX is well positioned to support the goals of the BRAIN
Initiative Cell Atlas Network (BICAN) and future efforts to extend these
mapping strategies to the human brain.

\clearpage \newpage

\section{Methods}\label{methods}

The following methods are all available as part of the ANTsX ecosystem
with analogous elements existing in both ANTsR (ANTs in R) and ANTsPy
(ANTs in Python), underpinned by a shared ANTs/ITK C++ core. Most
development for the work described was performed using ANTsPy. For
equivalent functionality in ANTsR, we refer the reader to the
comprehensive ANTsX tutorial: \url{https://tinyurl.com/antsxtutorial}.

\subsection{General ANTsX utilities}\label{general-antsx-utilities}

Although focused on distinct data types, the three pipelines presented
in this work share common components that address general challenges in
mapping mouse brain data. These include correcting image intensity
artifacts, denoising, spatial registration, template generation, and
visualization. Table \ref{table:methods} provides a concise summary of
the relevant ANTsX functionality.

\input{antsx_functionality_table}

\textbf{Preprocessing: bias field correction and denoising.} Standard
preprocessing steps in mouse brain imaging include correcting for
spatial intensity inhomogeneities and reducing image noise, both of
which can impact registration accuracy and downstream analysis. ANTsX
provides implementations of widely used methods for these tasks. The N4
bias field correction algorithm\textsuperscript{51}, originally
developed in ANTs and contributed to ITK, mitigates artifactual,
low-frequency intensity variation and is accessible via
\texttt{ants.n4\_bias\_field\_correction(...)}. Patch-based
denoising\textsuperscript{60} has been implemented as
\texttt{ants.denoise\_image(...)}.

\textbf{Image registration.} ANTsX includes a robust and flexible
framework for pairwise and groupwise image
registration\textsuperscript{86}. At its core is the SyN
algorithm\textsuperscript{50}, a symmetric diffeomorphic model with
optional B-spline regularization\textsuperscript{66}. In ANTsPy,
registration is performed via \texttt{ants.registration(...)} using
preconfigured parameter sets (e.g.,
\texttt{antsRegistrationSyNQuick{[}s{]}},
\texttt{antsRegistrationSyN{[}s{]}}) suitable for different imaging
modalities and levels of computational demand. Resulting transformations
can be applied to new images with \texttt{ants.apply\_transforms(...)}.

\textbf{Template generation.} ANTsX supports population-based template
generation through iterative pairwise registration to an evolving
estimate of the mean shape and intensity reference space across
subjects\textsuperscript{58}. This functionality was used in generating
the DevCCF templates\textsuperscript{16}. The procedure, implemented as
\texttt{ants.build\_template(...)}, produces average images in both
shape and intensity by aligning all inputs to a common evolving
template.

\textbf{Visualization.} To support visual inspection and quality
control, ANTsPy provides flexible image visualization with
\texttt{ants.plot(...)}. This function enables multi-slice and
multi-orientation rendering with optional overlays and label maps.

\subsection{Mapping fMOST data to
AllenCCFv3}\label{mapping-fmost-data-to-allenccfv3}

\textbf{Preprocessing.} Mapping fMOST data into the AllenCCFv3 presents
unique challenges due to its native ultra-high resolution and imaging
artifacts common to the fMOST modality. Each fMOST image can exceed a
terabyte in size, with spatial resolutions far exceeding those of the
AllenCCFv3 (25\,\(\mu m\) isotropic). To reduce computational burden and
prevent resolution mismatch, each fMOST image is downsampled using cubic
B-spline interpolation via \texttt{ants.resample\_image(...)} to match
the template resolution.

Stripe artifacts (i.e., periodic intensity distortions caused by
nonuniform sectioning or illumination) are common in fMOST and can
mislead deformable registration algorithms. These were removed using a
custom 3D notch filter (\texttt{remove\_stripe\_artifact(...)})
implemented in the \texttt{CCFAlignmentToolkit} using SciPy frequency
domain filtering. The filter targets dominant stripe frequencies along a
user-specified axis in the Fourier domain. In addition, intensity
inhomogeneity across sections, often arising from variable staining or
illumination, was corrected using N4 bias field correction.

\textbf{Template-based spatial normalization.} To facilitate
reproducible mapping, we first constructed a contralaterally symmetric
average template from 30 fMOST brains and their mirrored counterparts
using ANTsX template-building tools. Because the AllenCCFv3 and fMOST
data differ substantially in both intensity contrast and morphology,
direct deformable registration between individual fMOST brains and the
AllenCCFv3 was insufficiently robust. Instead, we performed a one-time
expert-guided label-driven registration between the average fMOST
template and AllenCCFv3. This involved sequential alignment of seven
manually selected anatomical regions: 1) brain mask/ventricles, 2)
caudate/putamen, 3) fimbria, 4) posterior choroid615 plexus, 5) optic
chiasm, 6) anterior choroid plexus, and 7) habenular commissure which
were prioritized to enable coarse-to-fine correction of shape
differences. Once established, this fMOST-template-to-AllenCCFv3
transform was reused for all subsequent specimens. Each new fMOST brain
was then registered to the average fMOST template using intensity-based
registration, followed by concatenation of transforms to produce the
final mapping into AllenCCFv3 space.

\textbf{Mapping neuron projections.} A key advantage of fMOST imaging is
its ability to support single neuron projection reconstruction across
the entire brain\textsuperscript{79}. Because these reconstructions are
stored as 3D point sets aligned to the original fMOST volume, we applied
the same composite transform used for image alignment to the point data
using ANTsX functionality. This enables seamless integration of cellular
morphology data into AllenCCFv3 space, facilitating comparative analyses
across specimens.

\subsection{Mapping MERFISH data to
AllenCCFv3}\label{mapping-merfish-data-to-allenccfv3}

\textbf{Preprocessing.} MERFISH data are acquired as a series of 2D
tissue sections, each comprising spatially localized gene expression
measurements at subcellular resolution. To enable 3D mapping to the
AllenCCFv3, we first constructed anatomical reference images by
aggregating the number of detected transcripts per voxel across all
probes within each section. These 2D projections were resampled to a
resolution of 10 \(\mu m\)\,\(\times\)\,10\,\(\mu m\) to match the
in-plane resolution of the AllenCCFv3.

Sections were coarsely aligned using manually annotated dorsal and
ventral midline points, allowing initial volumetric reconstruction.
However, anatomical fidelity remained limited by variation in section
orientation, spacing, and tissue loss. To further constrain alignment
and enable deformable registration, we derived region-level anatomical
labels directly from the gene expression data.

\textbf{Label creation.} We assigned each detected cell to one of 15
coarse anatomical regions (e.g., hippocampus, cortex, striatum---using
transcriptomic similarity to scRNA) seq reference data. These
assignments were aggregated across spatial grids to produce
probabilistic label maps for each section. To ensure full regional
coverage, morphological dilation was applied to fill gaps between
sparsely distributed cells. Finer-resolution structures (e.g., cortical
layers, habenula) were similarly labeled using marker gene enrichment
and spatial constraints. This dual-level labeling (i.e., coarse and
fine) allowed us to construct a robust anatomical scaffold in the
MERFISH coordinate system that could be matched to AllenCCFv3
annotations.

\textbf{Section matching via global alignment.} A major challenge was
compensating for oblique cutting angles and non-uniform section
thickness, which distort the anatomical shape and spacing of the
reconstructed volume. Rather than directly warping the MERFISH data into
atlas space, we globally aligned the AllenCCFv3 to the MERFISH
coordinate system. This was done via an affine transformation followed
by resampling of AllenCCFv3 sections to match the number and orientation
of MERFISH sections. This approach minimizes interpolation artifacts in
the MERFISH data and facilitates one-to-one section matching.

\textbf{Landmark-driven deformable alignment.} We used a 2.5D approach
for fine alignment of individual sections. In each MERFISH slice,
deformable registration was driven by sequential alignment of anatomical
landmarks between the label maps derived from MERFISH and AllenCCFv3. A
total of nine regions---including isocortical layers 2/3, 5, and 6, the
striatum, hippocampus, thalamus, and medial/lateral habenula---were
registered in an empirically determined order. After each round,
anatomical alignment was visually assessed by an expert, and the next
structure was selected to maximize improvement in the remaining
misaligned regions.

The final transform for each section combined the global affine
alignment and the per-structure deformable registrations. These were
concatenated to generate a 3D mapping from the original MERFISH space to
the AllenCCFv3 coordinate system. Once established, the composite
mapping enables direct transfer of gene-level and cell-type data from
MERFISH into atlas space, allowing integration with other imaging and
annotation datasets.

\subsection{DevCCF velocity flow transformation
model}\label{devccf-velocity-flow-transformation-model}

The Developmental Common Coordinate Framework
(DevCCF)\textsuperscript{16} provides a discrete set of age-specific
templates that temporally sample the developmental trajectory. To model
this biological progression more continuously, we introduce a velocity
flow--based paradigm for inferring diffeomorphic transformations between
developmental stages. This enables anatomically plausible estimation of
intermediate templates or mappings at arbitrary timepoints between the
E11.5 and P56 endpoints of the DevCCF. Our approach builds on
established insights from time-varying diffeomorphic
registration\textsuperscript{65}, where a velocity field governs the
smooth deformation of anatomical structures over time. Importantly, the
framework is extensible and can naturally accommodate additional
timepoints for the potential expansion of the DevCCF.

\textbf{Point sampling and region correspondence.} We first coalesced
the anatomical labels across the seven DevCCF templates (E11.5, E13.5,
E15.5, E18.5, P4, P14, P56) into 26 common structures that could be
consistently identified across development. These include major brain
regions such as the cortex, cerebellum, hippocampus, midbrain, and
ventricles. For each successive pair of templates, we performed
multi-label deformable registration using ANTsX to generate forward and
inverse transforms between anatomical label volumes. From the P56 space,
we randomly sampled approximately 1e6 points within and along the
boundaries of each labeled region and propagated them through each
pairwise mapping step (e.g., P56 \(\rightarrow\) P14, P14
\(\rightarrow\) P4, \ldots, E13.5 \(\rightarrow\) E11.5). This procedure
created time-indexed point sets tracing the spatial evolution of each
region.

\textbf{Velocity field fitting.} Using these point sets, we fit a
continuous velocity field over developmental time using a generalized
B-spline scattered data approximation method {[}\textsuperscript{85}{]},
which is implemented in ANTsX and ITK. The field was parameterized over
a log-scaled time axis to ensure finer temporal resolution during early
embryonic stages, where morphological changes are most rapid.
Optimization proceeded for approximately 125 iterations, minimizing the
average Euclidean norm between transformed points at each step. Ten
integration points were used to ensure numerical stability. The result
is a smooth, differentiable vector field that defines a diffeomorphic
transform between any two timepoints within the template range.

\textbf{Applications and availability.} This velocity model can be used
to estimate spatial transformations between any pair of developmental
stages---even those for which no empirical template exists---allowing
researchers to create interpolated atlases, align new datasets, or
measure continuous structural changes. It also enables developmental
alignment of multi-modal data (e.g., MRI to LSFM) by acting as a
unifying spatiotemporal scaffold. The implementation is accessible via
\texttt{ants.fit\_time\_varying\_transform\_to\_point\_sets(...)} in
ANTsPy, and we include reproducible examples in our public codebase.

\subsection{ANTsXNet mouse brain
applications}\label{antsxnet-mouse-brain-applications}

To support template-based deep learning approaches for structural brain
extraction and parcellation, we implemented dedicated pipelines using
the ANTsXNet framework. ANTsXNet comprises open-source deep learning
libraries in both Python (ANTsPyNet) and R (ANTsRNet) that interface
with the broader ANTsX ecosystem and are built on TensorFlow/Keras. Our
mouse brain pipelines mirror existing ANTsXNet tools for human imaging
but are adapted for species-specific anatomical variation, lower SNR,
and heterogeneous acquisition protocols.

\subsubsection{Deep learning training
setup}\label{deep-learning-training-setup}

All networks were implemented in ANTsPyNet using standard 3D U-net
architectures {[}\textsuperscript{87}{]}. Training was performed on an
NVIDIA DGX system (4 Tesla V100 GPUs, 256\,GB RAM). Model weights and
preprocessing routines are shared across ANTsPyNet and ANTsRNet to
ensure reproducibility and language portability. Public training scripts
and data generators are available at
\textbf{\url{https://github.com/ntustison/ANTsXNetTraining}}.

\textbf{Data augmentation.} Robust data augmentation was critical to
generalization across scanners, contrast types, and resolutions. We
applied both intensity- and shape-based augmentation strategies:

\begin{itemize}
\item
  \emph{Intensity augmentations:}

  \begin{itemize}
  \tightlist
  \item
    Gaussian, Poisson, and salt-and-pepper noise:
    \texttt{ants.add\_noise\_to\_image(...)}
  \item
    Simulated intensity inhomogeneity via bias field modeling:
    \texttt{antspynet.simulate\_bias\_field(...)}
  \item
    Histogram warping to simulate contrast variation:
    \texttt{antspynet.histogram\_warp\_image\_intensities(...)}
  \end{itemize}
\item
  \emph{Shape augmentations:}

  \begin{itemize}
  \tightlist
  \item
    Random nonlinear deformations and affine transforms:
    \texttt{antspynet.randomly\_transform\_image\_data(...)}
  \item
    Anisotropic resampling across axial, sagittal, and coronal planes:
    \texttt{ants.resample\_image(...)}
  \end{itemize}
\end{itemize}

\subsubsection{Brain extraction}\label{brain-extraction}

We trained a mouse-specific brain extraction model on two manually
masked T2-weighted templates, generated from public datasets
{[}\textsuperscript{68};\textsuperscript{67}{]}. One of the templates
was constructed from orthogonal 2D acquisitions using B-spline-based
volumetric synthesis via
\texttt{ants.fit\_bspline\_object\_to\_scattered\_data(...)}. Normalized
gradient magnitude was used as a weighting function to emphasize
boundaries during reconstruction.

This training strategy provides strong spatial priors despite limited
data --- leveraging high-quality template data and aggressive
augmentation to mimic population variability. The final trained network
is available via ANTsXNet:

\begin{itemize}
\tightlist
\item
  Template:
  \texttt{antspynet.get\_antsxnet\_data("bsplineT2MouseTemplate")}
\item
  Brain mask:
  \texttt{antspynet.get\_antsxnet\_data("bsplineT2MouseTemplateBrainMask")}
\end{itemize}

\subsubsection{Brain parcellation}\label{brain-parcellation}

For brain parcellation, we trained a 3D U-net model using the DevCCF P56
T2-weighted template and anatomical segmentations derived from
AllenCCFv3. This template-based training strategy allows the model to
produce accurate, multi-region parcellations without requiring
large-scale annotated subject data.

To harmonize intensity across specimens, input images were preprocessed
using rank-based intensity normalization
(\texttt{ants.rank\_intensity(...)}). Spatial harmonization was achieved
by affine and deformable alignment of each extracted brain to the P56
template prior to inference. In addition to the normalized image input,
the network also receives prior probability maps derived from the atlas
segmentations, providing additional spatial context. These resources are
available via \texttt{get\_antsxnet\_data(...)}.

\subsubsection{Evaluation and reuse}\label{evaluation-and-reuse}

To assess model generalizability, both brain extraction and parcellation
models were evaluated on an external longitudinal dataset
{[}\textsuperscript{69}{]} with varied scanning parameters. The pipeline
demonstrated robust performance without retraining, highlighting the
utility of a template-driven, low-shot approach. All models, training
scripts, and data resources are publicly available and designed for
plug-and-play use within ANTsX workflows.

\clearpage

\section*{Data availability}\label{data-availability}
\addcontentsline{toc}{section}{Data availability}

All data and software used in this work are publicly available. The
DevCCF atlas is available at \url{https://kimlab.io/brain-map/DevCCF/}.
ANTsPy, ANTsR, ANTsPyNet, and ANTsRNet are available through GitHub at
the ANTsX Ecosystem (\url{https://github.com/ANTsX}). Training scripts
for all deep learning functionality in ANTsXNet can also be found on
GitHub (\url{https://github.com/ntustison/ANTsXNetTraining}). A GitHub
repository specifically pertaining to the AllenCCFv3 mapping is
available at \url{https://github.com/dontminchenit/CCFAlignmentToolkit}.
For the other two contributions contained in this work, the longitudinal
DevCCF mapping and mouse cortical thickness pipeline, we refer the
interested reader to
\url{https://github.com/ntustison/ANTsXMouseBrainMapping}.

\clearpage

\section*{Acknowledgments}\label{acknowledgments}
\addcontentsline{toc}{section}{Acknowledgments}

Support for the research reported in this work includes funding from the
National Institute of Biomedical Imaging and Bioengineering
(R01-EB031722) and National Institute of Mental Health (RF1-MH124605 and
U24-MH114827).

We also acknowledge the data contribution of Dr.~Adam Raikes (GitHub
@araikes) of the Center for Innovation in Brain Science at the
University of Arizona for refining the weights of the mouse brain
extraction network.

\clearpage

\section*{Author contributions}\label{author-contributions}
\addcontentsline{toc}{section}{Author contributions}

N.T., M.C., and J.G. wrote the main manuscript text and figures. M.C.,
M.K., R.D., S.S., Q.W., L.G., J.D., C.G., and J.G. developed the Allen
registration pipelines. N.T. and F.K. developed the time-varying
velocity transformation model for the DevCCF. N.T. and M.T. developed
the brain parcellation and cortical thickness methodology. All authors
reviewed the manuscript. \clearpage

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{0}{0}
\bibitem[\citeproctext]{ref-Keller:2015aa}
\CSLLeftMargin{1. }%
\CSLRightInline{Keller, P. J. \& Ahrens, M. B.
\href{https://doi.org/10.1016/j.neuron.2014.12.039}{Visualizing
whole-brain activity and development at the single-cell level using
light-sheet microscopy}. \emph{Neuron} \textbf{85}, 462--83 (2015).}

\bibitem[\citeproctext]{ref-La-Manno:2021aa}
\CSLLeftMargin{2. }%
\CSLRightInline{La Manno, G. \emph{et al.}
\href{https://doi.org/10.1038/s41586-021-03775-x}{Molecular architecture
of the developing mouse brain}. \emph{Nature} \textbf{596}, 92--96
(2021).}

\bibitem[\citeproctext]{ref-Wen:2022aa}
\CSLLeftMargin{3. }%
\CSLRightInline{Wen, L. \emph{et al.}
\href{https://doi.org/10.1016/j.xinn.2022.100342}{Single-cell
technologies: From research to application}. \emph{Innovation (Camb)}
\textbf{3}, 100342 (2022).}

\bibitem[\citeproctext]{ref-Oh:2014aa}
\CSLLeftMargin{4. }%
\CSLRightInline{Oh, S. W. \emph{et al.}
\href{https://doi.org/10.1038/nature13186}{A mesoscale connectome of the
mouse brain}. \emph{Nature} \textbf{508}, 207--14 (2014).}

\bibitem[\citeproctext]{ref-Gong:2013aa}
\CSLLeftMargin{5. }%
\CSLRightInline{Gong, H. \emph{et al.}
\href{https://doi.org/10.1016/j.neuroimage.2013.02.005}{Continuously
tracing brain-wide long-distance axonal projections in mice at a
one-micron voxel resolution}. \emph{Neuroimage} \textbf{74}, 87--98
(2013).}

\bibitem[\citeproctext]{ref-Li:2010aa}
\CSLLeftMargin{6. }%
\CSLRightInline{Li, A. \emph{et al.}
\href{https://doi.org/10.1126/science.1191776}{Micro-optical sectioning
tomography to obtain a high-resolution atlas of the mouse brain}.
\emph{Science} \textbf{330}, 1404--8 (2010).}

\bibitem[\citeproctext]{ref-Ueda:2020aa}
\CSLLeftMargin{7. }%
\CSLRightInline{Ueda, H. R. \emph{et al.}
\href{https://doi.org/10.1038/s41583-019-0250-1}{Tissue clearing and its
applications in neuroscience}. \emph{Nat Rev Neurosci} \textbf{21},
61--79 (2020).}

\bibitem[\citeproctext]{ref-Stahl:2016aa}
\CSLLeftMargin{8. }%
\CSLRightInline{Sthl, P. L. \emph{et al.}
\href{https://doi.org/10.1126/science.aaf2403}{Visualization and
analysis of gene expression in tissue sections by spatial
transcriptomics}. \emph{Science} \textbf{353}, 78--82 (2016).}

\bibitem[\citeproctext]{ref-Burgess:2019aa}
\CSLLeftMargin{9. }%
\CSLRightInline{Burgess, D. J.
\href{https://doi.org/10.1038/s41576-019-0129-z}{Spatial transcriptomics
coming of age}. \emph{Nat Rev Genet} \textbf{20}, 317 (2019).}

\bibitem[\citeproctext]{ref-hardwick:2022aa}
\CSLLeftMargin{10. }%
\CSLRightInline{Hardwick, S. A. \emph{et al.} Single-nuclei isoform RNA
sequencing unlocks barcoded exon connectivity in frozen brain tissue.
\emph{Nature biotechnology} \textbf{40}, 1082--1092 (2022).}

\bibitem[\citeproctext]{ref-hawrylycz:2023aa}
\CSLLeftMargin{11. }%
\CSLRightInline{Hawrylycz, M. \emph{et al.} A guide to the BRAIN
initiative cell census network data ecosystem. \emph{PLoS biology}
\textbf{21}, e3002133 (2023).}

\bibitem[\citeproctext]{ref-Wang:2020aa}
\CSLLeftMargin{12. }%
\CSLRightInline{Wang, Q. \emph{et al.}
\href{https://doi.org/10.1016/j.cell.2020.04.007}{The allen mouse brain
common coordinate framework: A 3D reference atlas}. \emph{Cell}
\textbf{181}, 936--953.e20 (2020).}

\bibitem[\citeproctext]{ref-perens:2021aa}
\CSLLeftMargin{13. }%
\CSLRightInline{Perens, J. \emph{et al.} An optimized mouse brain atlas
for automated mapping and quantification of neuronal activity using
iDISCO+ and light sheet fluorescence microscopy. \emph{Neuroinformatics}
\textbf{19}, 433--446 (2021).}

\bibitem[\citeproctext]{ref-ma:2005aa}
\CSLLeftMargin{14. }%
\CSLRightInline{Ma, Y. \emph{et al.} A three-dimensional digital atlas
database of the adult C57BL/6J mouse brain by magnetic resonance
microscopy. \emph{Neuroscience} \textbf{135}, 1203--1215 (2005).}

\bibitem[\citeproctext]{ref-qu:2022aa}
\CSLLeftMargin{15. }%
\CSLRightInline{Qu, L. \emph{et al.} Cross-modal coherent registration
of whole mouse brains. \emph{Nature Methods} \textbf{19}, 111--118
(2022).}

\bibitem[\citeproctext]{ref-Kronman:2024aa}
\CSLLeftMargin{16. }%
\CSLRightInline{Kronman, F. N. \emph{et al.}
\href{https://doi.org/10.1038/s41467-024-53254-w}{Developmental mouse
brain common coordinate framework}. \emph{Nat Commun} \textbf{15}, 9072
(2024).}

\bibitem[\citeproctext]{ref-chuang:2011aa}
\CSLLeftMargin{17. }%
\CSLRightInline{Chuang, N. \emph{et al.} An MRI-based atlas and database
of the developing mouse brain. \emph{Neuroimage} \textbf{54}, 80--89
(2011).}

\bibitem[\citeproctext]{ref-dries:2021aa}
\CSLLeftMargin{18. }%
\CSLRightInline{Dries, R. \emph{et al.} Advances in spatial
transcriptomic data analysis. \emph{Genome research} \textbf{31},
1706--1718 (2021).}

\bibitem[\citeproctext]{ref-ricci:2022aa}
\CSLLeftMargin{19. }%
\CSLRightInline{Ricci, P. \emph{et al.} Removing striping artifacts in
light-sheet fluorescence microscopy: A review. \emph{Progress in
biophysics and molecular biology} \textbf{168}, 52--65 (2022).}

\bibitem[\citeproctext]{ref-agarwal:2016aa}
\CSLLeftMargin{20. }%
\CSLRightInline{Agarwal, N., Xu, X. \& Gopi, M. Robust registration of
mouse brain slices with severe histological artifacts. in
\emph{Proceedings of the tenth indian conference on computer vision,
graphics and image processing} 1--8 (2016).}

\bibitem[\citeproctext]{ref-agarwal:2017aa}
\CSLLeftMargin{21. }%
\CSLRightInline{Agarwal, N., Xu, X. \& Gopi, M. Automatic detection of
histological artifacts in mouse brain slice images. in \emph{Medical
computer vision and bayesian and graphical models for biomedical
imaging: MICCAI 2016 international workshops, MCV and BAMBI, athens,
greece, october 21, 2016, revised selected papers 8} 105--115 (Springer,
2017).}

\bibitem[\citeproctext]{ref-tward:2019aa}
\CSLLeftMargin{22. }%
\CSLRightInline{Tward, D. \emph{et al.} 3d mapping of serial histology
sections with anomalies using a novel robust deformable registration
algorithm. in \emph{International workshop on multimodal brain image
analysis} 162--173 (Springer, 2019).}

\bibitem[\citeproctext]{ref-cahill:2012aa}
\CSLLeftMargin{23. }%
\CSLRightInline{Cahill, L. S. \emph{et al.} Preparation of fixed mouse
brains for MRI. \emph{Neuroimage} \textbf{60}, 933--939 (2012).}

\bibitem[\citeproctext]{ref-sunkin:2012}
\CSLLeftMargin{24. }%
\CSLRightInline{Sunkin, S. M. \emph{et al.} Allen brain atlas: An
integrated spatio-temporal portal for exploring the central nervous
system. \emph{Nucleic acids research} \textbf{41}, D996--D1008 (2012).}

\bibitem[\citeproctext]{ref-kim:2017aa}
\CSLLeftMargin{25. }%
\CSLRightInline{Kim, Y. \emph{et al.} Brain-wide maps reveal stereotyped
cell-type-based cortical architecture and subcortical sexual dimorphism.
\emph{Cell} \textbf{171}, 456--469 (2017).}

\bibitem[\citeproctext]{ref-Furth:2018aa}
\CSLLeftMargin{26. }%
\CSLRightInline{Frth, D. \emph{et al.}
\href{https://doi.org/10.1038/s41593-017-0027-7}{An interactive
framework for whole-brain maps at cellular resolution}. \emph{Nat
Neurosci} \textbf{21}, 139--149 (2018).}

\bibitem[\citeproctext]{ref-li:2022aa}
\CSLLeftMargin{27. }%
\CSLRightInline{Li, Y. \emph{et al.} mBrainAligner-web: A web server for
cross-modal coherent registration of whole mouse brains.
\emph{Bioinformatics} \textbf{38}, 4654--4655 (2022).}

\bibitem[\citeproctext]{ref-puchades:2019aa}
\CSLLeftMargin{28. }%
\CSLRightInline{Puchades, M. A., Csucs, G., Ledergerber, D., Leergaard,
T. B. \& Bjaalie, J. G. Spatial registration of serial microscopic brain
images to three-dimensional reference atlases with the QuickNII tool.
\emph{PloS one} \textbf{14}, e0216796 (2019).}

\bibitem[\citeproctext]{ref-eastwood:2019aa}
\CSLLeftMargin{29. }%
\CSLRightInline{Eastwood, B. S. \emph{et al.} Whole mouse brain
reconstruction and registration to a reference atlas with standard
histochemical processing of coronal sections. \emph{Journal of
Comparative Neurology} \textbf{527}, 2170--2178 (2019).}

\bibitem[\citeproctext]{ref-Ni:2020aa}
\CSLLeftMargin{30. }%
\CSLRightInline{Ni, H. \emph{et al.}
\href{https://doi.org/10.1038/s41598-020-59042-y}{A robust image
registration interface for large volume brain atlas}. \emph{Sci Rep}
\textbf{10}, 2139 (2020).}

\bibitem[\citeproctext]{ref-Pallast:2019aa}
\CSLLeftMargin{31. }%
\CSLRightInline{Pallast, N. \emph{et al.}
\href{https://doi.org/10.3389/fninf.2019.00042}{Processing pipeline for
atlas-based imaging data analysis of structural and functional mouse
brain MRI (AIDAmri)}. \emph{Front Neuroinform} \textbf{13}, 42 (2019).}

\bibitem[\citeproctext]{ref-Celestine:2020aa}
\CSLLeftMargin{32. }%
\CSLRightInline{Celestine, M., Nadkarni, N. A., Garin, C. M., Bougacha,
S. \& Dhenain, M.
\href{https://doi.org/10.3389/fninf.2020.00024}{Sammba-MRI: A library
for processing SmAll-MaMmal BrAin MRI data in python}. \emph{Front
Neuroinform} \textbf{14}, 24 (2020).}

\bibitem[\citeproctext]{ref-Ioanas:2021aa}
\CSLLeftMargin{33. }%
\CSLRightInline{Ioanas, H.-I., Marks, M., Zerbi, V., Yanik, M. F. \&
Rudin, M. \href{https://doi.org/10.1016/j.neuroimage.2021.118386}{An
optimized registration workflow and standard geometric space for small
animal brain imaging}. \emph{Neuroimage} \textbf{241}, 118386 (2021).}

\bibitem[\citeproctext]{ref-aggarwal:2009aa}
\CSLLeftMargin{34. }%
\CSLRightInline{Aggarwal, M., Zhang, J., Miller, M. I., Sidman, R. L. \&
Mori, S. Magnetic resonance imaging and micro-computed tomography
combined atlas of developing and adult mouse brains for stereotaxic
surgery. \emph{Neuroscience} \textbf{162}, 1339--1350 (2009).}

\bibitem[\citeproctext]{ref-chandrashekhar:2021aa}
\CSLLeftMargin{35. }%
\CSLRightInline{Chandrashekhar, V. \emph{et al.} CloudReg: Automatic
terabyte-scale cross-modal brain volume registration. \emph{Nature
methods} \textbf{18}, 845--846 (2021).}

\bibitem[\citeproctext]{ref-Jin:2022aa}
\CSLLeftMargin{36. }%
\CSLRightInline{Jin, M. \emph{et al.}
\href{https://doi.org/10.1523/ENEURO.0482-21.2022}{SMART: An open-source
extension of WholeBrain for intact mouse brain registration and
segmentation}. \emph{eNeuro} \textbf{9}, (2022).}

\bibitem[\citeproctext]{ref-Negwer:2022aa}
\CSLLeftMargin{37. }%
\CSLRightInline{Negwer, M. \emph{et al.}
\href{https://doi.org/10.1093/gigascience/giad035}{FriendlyClearMap: An
optimized toolkit for mouse brain mapping and analysis}.
\emph{Gigascience} \textbf{12}, (2022).}

\bibitem[\citeproctext]{ref-lin:2023aa}
\CSLLeftMargin{38. }%
\CSLRightInline{Lin, W. \emph{et al.} Whole-brain mapping of
histaminergic projections in mouse brain. \emph{Proceedings of the
National Academy of Sciences} \textbf{120}, e2216231120 (2023).}

\bibitem[\citeproctext]{ref-zhang:2021aa}
\CSLLeftMargin{39. }%
\CSLRightInline{Zhang, M. \emph{et al.} Spatially resolved cell atlas of
the mouse primary motor cortex by MERFISH. \emph{Nature} \textbf{598},
137--143 (2021).}

\bibitem[\citeproctext]{ref-shi:2023aa}
\CSLLeftMargin{40. }%
\CSLRightInline{Shi, H. \emph{et al.} Spatial atlas of the mouse central
nervous system at molecular resolution. \emph{Nature} \textbf{622},
552--561 (2023).}

\bibitem[\citeproctext]{ref-zhang:2023aa}
\CSLLeftMargin{41. }%
\CSLRightInline{Zhang, Y. \emph{et al.} Reference-based cell type
matching of in situ image-based spatial transcriptomics data on primary
visual cortex of mouse brain. \emph{Scientific Reports} \textbf{13},
9567 (2023).}

\bibitem[\citeproctext]{ref-Klein:2010aa}
\CSLLeftMargin{42. }%
\CSLRightInline{Klein, S., Staring, M., Murphy, K., Viergever, M. A. \&
Pluim, J. P. W. \href{https://doi.org/10.1109/TMI.2009.2035616}{Elastix:
A toolbox for intensity-based medical image registration}. \emph{IEEE
Trans Med Imaging} \textbf{29}, 196--205 (2010).}

\bibitem[\citeproctext]{ref-fedorov:2012aa}
\CSLLeftMargin{43. }%
\CSLRightInline{Fedorov, A. \emph{et al.} 3D slicer as an image
computing platform for the quantitative imaging network. \emph{Magnetic
resonance imaging} \textbf{30}, 1323--1341 (2012).}

\bibitem[\citeproctext]{ref-Tustison:2021aa}
\CSLLeftMargin{44. }%
\CSLRightInline{Tustison, N. J. \emph{et al.}
\href{https://doi.org/10.1038/s41598-021-87564-6}{The ANTsX ecosystem
for quantitative biological and medical imaging}. \emph{Sci Rep}
\textbf{11}, 9068 (2021).}

\bibitem[\citeproctext]{ref-Rolfe:2023aa}
\CSLLeftMargin{45. }%
\CSLRightInline{Rolfe, S. M., Whikehart, S. M. \& Maga, A. M.
\href{https://doi.org/10.1242/bio.059698}{Deep learning enabled
multi-organ segmentation of mouse embryos}. \emph{Biol Open}
\textbf{12}, bio059698 (2023).}

\bibitem[\citeproctext]{ref-pagani:2016aa}
\CSLLeftMargin{46. }%
\CSLRightInline{Pagani, M., Damiano, M., Galbusera, A., Tsaftaris, S. A.
\& Gozzi, A. Semi-automated registration-based anatomical labelling,
voxel based morphometry and cortical thickness mapping of the mouse
brain. \emph{Journal of neuroscience methods} \textbf{267}, 62--73
(2016).}

\bibitem[\citeproctext]{ref-Anderson:2019aa}
\CSLLeftMargin{47. }%
\CSLRightInline{Anderson, R. J. \emph{et al.}
\href{https://doi.org/10.1007/s12021-018-9410-0}{Small animal
multivariate brain analysis (SAMBA) - a high throughput pipeline with a
validation framework}. \emph{Neuroinformatics} \textbf{17}, 451--472
(2019).}

\bibitem[\citeproctext]{ref-allan:2019aa}
\CSLLeftMargin{48. }%
\CSLRightInline{Allan Johnson, G. \emph{et al.} Whole mouse brain
connectomics. \emph{Journal of Comparative Neurology} \textbf{527},
2146--2157 (2019).}

\bibitem[\citeproctext]{ref-Yao:2023aa}
\CSLLeftMargin{49. }%
\CSLRightInline{Yao, Z. \emph{et al.}
\href{https://doi.org/10.1038/s41586-023-06812-z}{A high-resolution
transcriptomic and spatial atlas of cell types in the whole mouse
brain}. \emph{Nature} \textbf{624}, 317--332 (2023).}

\bibitem[\citeproctext]{ref-Avants:2008aa}
\CSLLeftMargin{50. }%
\CSLRightInline{Avants, B. B., Epstein, C. L., Grossman, M. \& Gee, J.
C. \href{https://doi.org/10.1016/j.media.2007.06.004}{Symmetric
diffeomorphic image registration with cross-correlation: Evaluating
automated labeling of elderly and neurodegenerative brain}. \emph{Med
Image Anal} \textbf{12}, 26--41 (2008).}

\bibitem[\citeproctext]{ref-Tustison:2010ac}
\CSLLeftMargin{51. }%
\CSLRightInline{Tustison, N. J. \emph{et al.}
\href{https://doi.org/10.1109/TMI.2010.2046908}{{N4ITK}: Improved {N3}
bias correction}. \emph{IEEE Trans Med Imaging} \textbf{29}, 1310--20
(2010).}

\bibitem[\citeproctext]{ref-Bajcsy:1982aa}
\CSLLeftMargin{52. }%
\CSLRightInline{Bajcsy, R. \& Broit, C. Matching of deformed images. in
\emph{{S}ixth {I}nternational {C}onference on {P}attern {R}ecognition
({ICPR}'82)} 351--353 (1982).}

\bibitem[\citeproctext]{ref-Bajcsy:1989aa}
\CSLLeftMargin{53. }%
\CSLRightInline{Bajcsy, R. \& Kovacic, S.
\href{https://doi.org/10.1016/S0734-189X(89)80014-3}{Multiresolution
elastic matching}. \emph{Computer Vision, Graphics, and Image
Processing} \textbf{46}, 1--21 (1989).}

\bibitem[\citeproctext]{ref-Gee:1993aa}
\CSLLeftMargin{54. }%
\CSLRightInline{Gee, J. C., Reivich, M. \& Bajcsy, R.
\href{https://www.ncbi.nlm.nih.gov/pubmed/8454749}{Elastically deforming
3D atlas to match anatomical brain images}. \emph{J Comput Assist
Tomogr} \textbf{17}, 225--36 (1993).}

\bibitem[\citeproctext]{ref-Klein:2009aa}
\CSLLeftMargin{55. }%
\CSLRightInline{Klein, A. \emph{et al.}
\href{https://doi.org/10.1016/j.neuroimage.2008.12.037}{Evaluation of 14
nonlinear deformation algorithms applied to human brain {MRI}
registration}. \emph{Neuroimage} \textbf{46}, 786--802 (2009).}

\bibitem[\citeproctext]{ref-Murphy:2011aa}
\CSLLeftMargin{56. }%
\CSLRightInline{Murphy, K. \emph{et al.}
\href{https://doi.org/10.1109/TMI.2011.2158349}{Evaluation of
registration methods on thoracic {CT}: The {EMPIRE10} challenge}.
\emph{IEEE Trans Med Imaging} \textbf{30}, 1901--20 (2011).}

\bibitem[\citeproctext]{ref-Baheti:2021aa}
\CSLLeftMargin{57. }%
\CSLRightInline{Baheti, B. \emph{et al.}
\href{https://arxiv.org/abs/2112.06979}{The brain tumor sequence
registration challenge: Establishing correspondence between
pre-operative and follow-up MRI scans of diffuse glioma patients}.
(2021).}

\bibitem[\citeproctext]{ref-Avants:2010aa}
\CSLLeftMargin{58. }%
\CSLRightInline{Avants, B. B. \emph{et al.}
\href{https://doi.org/10.1016/j.neuroimage.2009.09.062}{The optimal
template effect in hippocampus studies of diseased populations}.
\emph{Neuroimage} \textbf{49}, 2457--66 (2010).}

\bibitem[\citeproctext]{ref-Avants:2011uf}
\CSLLeftMargin{59. }%
\CSLRightInline{Avants, B. B., Tustison, N. J., Wu, J., Cook, P. A. \&
Gee, J. C. \href{https://doi.org/10.1007/s12021-011-9109-y}{An open
source multivariate framework for n-tissue segmentation with evaluation
on public data}. \emph{Neuroinformatics} \textbf{9}, 381--400 (2011).}

\bibitem[\citeproctext]{ref-Manjon:2010aa}
\CSLLeftMargin{60. }%
\CSLRightInline{Manjn, J. V., Coup, P., Mart-Bonmat, L., Collins, D.
L. \& Robles, M. \href{https://doi.org/10.1002/jmri.22003}{Adaptive
non-local means denoising of {MR} images with spatially varying noise
levels}. \emph{J Magn Reson Imaging} \textbf{31}, 192--203 (2010).}

\bibitem[\citeproctext]{ref-Wang:2013ab}
\CSLLeftMargin{61. }%
\CSLRightInline{Wang, H. \emph{et al.}
\href{https://doi.org/10.1109/TPAMI.2012.143}{Multi-atlas segmentation
with joint label fusion}. \emph{IEEE Trans Pattern Anal Mach Intell}
\textbf{35}, 611--23 (2013).}

\bibitem[\citeproctext]{ref-Tustison:2014aa}
\CSLLeftMargin{62. }%
\CSLRightInline{Tustison, N. J. \emph{et al.} Optimal symmetric
multimodal templates and concatenated random forests for supervised
brain tumor segmentation (simplified) with {\(ANTsR\)}.
\emph{Neuroinformatics} (2014)
doi:\href{https://doi.org/10.1007/s12021-014-9245-2}{10.1007/s12021-014-9245-2}.}

\bibitem[\citeproctext]{ref-Tustison:2015ab}
\CSLLeftMargin{63. }%
\CSLRightInline{Tustison, N. J., Yang, Y. \& Salerno, M.
\href{https://doi.org/10.1007/978-3-319-14678-2_1}{Advanced
normalization tools for cardiac motion correction}. in \emph{Statistical
atlases and computational models of the heart - imaging and modelling
challenges} (eds. Camara, O. et al.) vol. 8896 3--12 (Springer
International Publishing, 2015).}

\bibitem[\citeproctext]{ref-McCormick:2014aa}
\CSLLeftMargin{64. }%
\CSLRightInline{McCormick, M., Liu, X., Jomier, J., Marion, C. \&
Ibanez, L. \href{https://doi.org/10.3389/fninf.2014.00013}{ITK: Enabling
reproducible research and open science}. \emph{Front Neuroinform}
\textbf{8}, 13 (2014).}

\bibitem[\citeproctext]{ref-Beg:2005aa}
\CSLLeftMargin{65. }%
\CSLRightInline{Beg, M. F., Miller, M. I., Trouv, A. \& Younes, L.
\href{https://doi.org/10.1023/B:VISI.0000043755.93987.aa}{Computing
large deformation metric mappings via geodesic flows of
diffeomorphisms}. \emph{International Journal of Computer Vision}
\textbf{61}, 139--157 (2005).}

\bibitem[\citeproctext]{ref-Tustison:2013ac}
\CSLLeftMargin{66. }%
\CSLRightInline{Tustison, N. J. \& Avants, B. B.
\href{https://doi.org/10.3389/fninf.2013.00039}{Explicit {B}-spline
regularization in diffeomorphic image registration}. \emph{Front
Neuroinform} \textbf{7}, 39 (2013).}

\bibitem[\citeproctext]{ref-Hsu2021}
\CSLLeftMargin{67. }%
\CSLRightInline{Hsu, L.-M. \emph{et al.} CAMRI mouse brain MRI data.}

\bibitem[\citeproctext]{ref-Reshetnikov2021}
\CSLLeftMargin{68. }%
\CSLRightInline{Reshetnikov, V. \emph{et al.} High-resolution MRI data
of brain C57BL/6 and BTBR mice in three different anatomical views.}

\bibitem[\citeproctext]{ref-Rahman:2023aa}
\CSLLeftMargin{69. }%
\CSLRightInline{Rahman, N., Xu, K., Budde, M. D., Brown, A. \& Baron, C.
A. \href{https://doi.org/10.1038/s41597-023-01942-5}{A longitudinal
microstructural MRI dataset in healthy C57Bl/6 mice at 9.4 tesla}.
\emph{Sci Data} \textbf{10}, 94 (2023).}

\bibitem[\citeproctext]{ref-Liu:2023aa}
\CSLLeftMargin{70. }%
\CSLRightInline{Liu, J. \emph{et al.}
\href{https://doi.org/10.26508/lsa.202201701}{Concordance of MERFISH
spatial transcriptomics with bulk and single-cell RNA sequencing}.
\emph{Life Sci Alliance} \textbf{6}, (2023).}

\bibitem[\citeproctext]{ref-Stringer:2021aa}
\CSLLeftMargin{71. }%
\CSLRightInline{Stringer, C., Wang, T., Michaelos, M. \& Pachitariu, M.
\href{https://doi.org/10.1038/s41592-020-01018-x}{Cellpose: A generalist
algorithm for cellular segmentation}. \emph{Nat Methods} \textbf{18},
100--106 (2021).}

\bibitem[\citeproctext]{ref-jia:2011aa}
\CSLLeftMargin{72. }%
\CSLRightInline{Jia, H., Yap, P.-T., Wu, G., Wang, Q. \& Shen, D.
Intermediate templates guided groupwise registration of diffusion tensor
images. \emph{NeuroImage} \textbf{54}, 928--939 (2011).}

\bibitem[\citeproctext]{ref-tang:2009aa}
\CSLLeftMargin{73. }%
\CSLRightInline{Tang, S., Fan, Y., Wu, G., Kim, M. \& Shen, D. RABBIT:
Rapid alignment of brains by building intermediate templates.
\emph{NeuroImage} \textbf{47}, 1277--1287 (2009).}

\bibitem[\citeproctext]{ref-dewey:2017aa}
\CSLLeftMargin{74. }%
\CSLRightInline{Dewey, B. E., Carass, A., Blitz, A. M. \& Prince, J. L.
Efficient multi-atlas registration using an intermediate template image.
in \emph{Proceedings of SPIE--the international society for optical
engineering} vol. 10137 (NIH Public Access, 2017).}

\bibitem[\citeproctext]{ref-perens:2023aa}
\CSLLeftMargin{75. }%
\CSLRightInline{Perens, J. \emph{et al.} Multimodal 3D mouse brain atlas
framework with the skull-derived coordinate system.
\emph{Neuroinformatics} \textbf{21}, 269--286 (2023).}

\bibitem[\citeproctext]{ref-Gong:2016aa}
\CSLLeftMargin{76. }%
\CSLRightInline{Gong, H. \emph{et al.}
\href{https://doi.org/10.1038/ncomms12142}{High-throughput dual-colour
precision imaging for brain-wide connectome with cytoarchitectonic
landmarks at the cellular level}. \emph{Nat Commun} \textbf{7}, 12142
(2016).}

\bibitem[\citeproctext]{ref-Wang:2021aa}
\CSLLeftMargin{77. }%
\CSLRightInline{Wang, J. \emph{et al.}
\href{https://doi.org/10.1007/s12264-020-00616-1}{Divergent projection
patterns revealed by reconstruction of individual neurons in
orbitofrontal cortex}. \emph{Neurosci Bull} \textbf{37}, 461--477
(2021).}

\bibitem[\citeproctext]{ref-Rotolo:2008aa}
\CSLLeftMargin{78. }%
\CSLRightInline{Rotolo, T., Smallwood, P. M., Williams, J. \& Nathans,
J.
\href{https://doi.org/10.1371/journal.pone.0004099}{Genetically-directed,
cell type-specific sparse labeling for the analysis of neuronal
morphology}. \emph{PLoS One} \textbf{3}, e4099 (2008).}

\bibitem[\citeproctext]{ref-Peng:2021aa}
\CSLLeftMargin{79. }%
\CSLRightInline{Peng, H. \emph{et al.}
\href{https://doi.org/10.1038/s41586-021-03941-1}{Morphological
diversity of single neurons in molecularly defined cell types}.
\emph{Nature} \textbf{598}, 174--181 (2021).}

\bibitem[\citeproctext]{ref-Kronman:2023aa}
\CSLLeftMargin{80. }%
\CSLRightInline{Kronman, F. A. \emph{et al.} Developmental mouse brain
common coordinate framework. \emph{bioRxiv} (2023)
doi:\href{https://doi.org/10.1101/2023.09.14.557789}{10.1101/2023.09.14.557789}.}

\bibitem[\citeproctext]{ref-Chon:2019aa}
\CSLLeftMargin{81. }%
\CSLRightInline{Chon, U., Vanselow, D. J., Cheng, K. C. \& Kim, Y.
\href{https://doi.org/10.1038/s41467-019-13057-w}{Enhanced and unified
anatomical labeling for a common mouse brain atlas}. \emph{Nat Commun}
\textbf{10}, 5067 (2019).}

\bibitem[\citeproctext]{ref-Tasic:2016aa}
\CSLLeftMargin{82. }%
\CSLRightInline{Tasic, B. \emph{et al.}
\href{https://doi.org/10.1038/nn.4216}{Adult mouse cortical cell
taxonomy revealed by single cell transcriptomics}. \emph{Nat Neurosci}
\textbf{19}, 335--46 (2016).}

\bibitem[\citeproctext]{ref-Bergmann:2020aa}
\CSLLeftMargin{83. }%
\CSLRightInline{Bergmann, E., Gofman, X., Kavushansky, A. \& Kahn, I.
\href{https://doi.org/10.1038/s42003-020-01472-5}{Individual variability
in functional connectivity architecture of the mouse brain}.
\emph{Commun Biol} \textbf{3}, 738 (2020).}

\bibitem[\citeproctext]{ref-Billot:2023aa}
\CSLLeftMargin{84. }%
\CSLRightInline{Billot, B. \emph{et al.}
\href{https://doi.org/10.1016/j.media.2023.102789}{SynthSeg:
Segmentation of brain MRI scans of any contrast and resolution without
retraining}. \emph{Med Image Anal} \textbf{86}, 102789 (2023).}

\bibitem[\citeproctext]{ref-Tustison:2006aa}
\CSLLeftMargin{85. }%
\CSLRightInline{Tustison, N. J. \& Amini, A. A.
\href{https://doi.org/10.1109/TMI.2005.861015}{Biventricular myocardial
strains via nonrigid registration of anatomical {NURBS} model
{[}corrected{]}}. \emph{IEEE Trans Med Imaging} \textbf{25}, 94--112
(2006).}

\bibitem[\citeproctext]{ref-Avants:2014aa}
\CSLLeftMargin{86. }%
\CSLRightInline{Avants, B. B. \emph{et al.}
\href{https://doi.org/10.3389/fninf.2014.00044}{The {Insight} {ToolKit}
image registration framework}. \emph{Front Neuroinform} \textbf{8}, 44
(2014).}

\bibitem[\citeproctext]{ref-Falk:2019aa}
\CSLLeftMargin{87. }%
\CSLRightInline{Falk, T. \emph{et al.}
\href{https://doi.org/10.1038/s41592-018-0261-2}{U-net: Deep learning
for cell counting, detection, and morphometry}. \emph{Nat Methods}
\textbf{16}, 67--70 (2019).}

\end{CSLReferences}

\end{document}
